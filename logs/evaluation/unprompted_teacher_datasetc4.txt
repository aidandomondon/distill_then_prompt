Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Fetching 2 files:  50%|█████     | 1/2 [01:09<01:09, 69.58s/it]Fetching 2 files: 100%|██████████| 2/2 [01:09<00:00, 34.79s/it]
Traceback (most recent call last):
  File "/Users/aidandomondon/distill-then-prompt/evaluate.py", line 111, in <module>
    model = GPTPromptTuningLM.from_pretrained(
        args.model_name_or_path,
    ...<3 lines>...
        local_files_only=local_files_only,
    )
  File "/Users/aidandomondon/distill-then-prompt/prompt/model_gpt.py", line 20, in from_pretrained
    model = super().from_pretrained(pretrained_model_name_or_path, **kwargs)
  File "/Users/aidandomondon/distill-then-prompt/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py", line 277, in _wrapper
    return func(*args, **kwargs)
  File "/Users/aidandomondon/distill-then-prompt/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py", line 5048, in from_pretrained
    ) = cls._load_pretrained_model(
        ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        model,
        ^^^^^^
    ...<12 lines>...
        weights_only=weights_only,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/aidandomondon/distill-then-prompt/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py", line 5432, in _load_pretrained_model
    caching_allocator_warmup(model, expanded_device_map, hf_quantizer)
    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/aidandomondon/distill-then-prompt/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py", line 6105, in caching_allocator_warmup
    _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)
RuntimeError: Invalid buffer size: 11.61 GiB
